{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6597bb3-15b6-4639-82ed-841386591567",
   "metadata": {},
   "source": [
    "![NCAR UCAR Logo](../NCAR_CISL_NSF_banner.jpeg)\n",
    "# Using Dask with GPUs via CuPy and introducing cuDF\n",
    "**NCAR GPU Workshop  \n",
    "August 11th 2022**\n",
    "\n",
    "**Presenter:**  \n",
    "Brian Vanderwende  \n",
    "[vanderwb@ucar.edu](mailto:vanderwb@ucar.edu)  \n",
    "Consulting Services Group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77cb026-61af-4635-b40d-3ed940d9c4c9",
   "metadata": {},
   "source": [
    "## Obtaining this notebook\n",
    "Follow these steps to create a clone of this repo. If you have already done so in a prior session, simply run a pull operation to obtain the materials for this week.\n",
    "\n",
    "| JupyterLab | Terminal |\n",
    "|-|-|\n",
    "| Use the JupyterHub GitHub GUI on the left</br>panel or the below shell commands | `git clone git@github.com:NCAR/GPU_workshop.git`</br>`git pull` |\n",
    "\n",
    "Once cloned or updated, navigate to `GPU_workshop/13_DaskGPU` in the repo and open this notebook.\n",
    "\n",
    "*For this session, we do not have the resources allocated for attendees to follow along during the live presentation, as we will demonstrate scaling and memory effects and thus need to use V100s. Instructions are provided at the end of this notebook for running it independently after the presentation.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300697eb-0d7a-4e09-80fd-869a2d502971",
   "metadata": {},
   "source": [
    "## Workshop Etiquette\n",
    "* Please mute yourself and turn off video during the session.\n",
    "* Questions may be submitted in the chat and will be answered when appropriate. You may also raise your hand, unmute, and ask questions during Q&A at the end of the presentation.\n",
    "* By participating, you are agreeing to [UCARâ€™s Code of Conduct](https://www.ucar.edu/who-we-are/ethics-integrity/codes-conduct/participants)\n",
    "* Recordings & other material will be archived & shared publicly.\n",
    "* Feel free to follow up with the GPU workshop team via Slack or submit support requests to [support.ucar.edu](https://support.ucar.edu)\n",
    "    * Office Hours: Asynchronous support via [Slack](https://ncargpuusers.slack.com) or schedule a time with an organizer\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d519440c-5da3-4d0a-a930-f56aa6a04d89",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Recall from last session...\n",
    "\n",
    "We explored the **CuPy** package, which provides a GPU drop-in replacement for *most* **Numpy** functionality ([comparison table](https://docs.cupy.dev/en/stable/reference/comparison.html)).\n",
    "\n",
    "### **CuPy**: creating arrays of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37561ab0-c0c0-4ec1-b06c-245daea3047d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy as cp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c1a336-bdc6-40e9-af47-fa1177a9438b",
   "metadata": {},
   "source": [
    "Creating a random Gaussian array on the CPU with Numpy ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bb7e8b-d094-405d-965d-173b132ea6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_arr = np.random.randn(10_000, 10_000)\n",
    "print(f'Mem used: {cpu_arr.nbytes / 1024**2:.2f} MiB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd7b37b-e910-4e33-9baa-5f64f3ad6c11",
   "metadata": {},
   "source": [
    "And creating the same array on the GPU ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c413692a-9734-4a36-9019-35a429bb8a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_arr = cp.random.randn(10_000, 10_000)\n",
    "print(f'Mem used: {gpu_arr.nbytes / 1024**2:.2f} MiB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafaa657-8746-4ebc-85ac-8f9c8cbd897b",
   "metadata": {},
   "source": [
    "#### Comparing the speed of `.mean()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51cad4d-2095-406f-b7cd-594851f7e393",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cpu_arr.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a221ca21-2934-4926-a90a-2c02c42d9ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gpu_arr.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78917b8-e146-4b96-8c94-15b324f1c5ae",
   "metadata": {},
   "source": [
    "#### Checking GPU results for correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f19a2e-5a47-43f2-972a-07dbd822236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gpu_copy = cp.array(cpu_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b9c0ae-910c-490c-9ab8-a5b875a4e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gpu_copy.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dec6c5f-0914-4add-bdcb-9a220106fe52",
   "metadata": {},
   "source": [
    "#### Analyzing CuPy's memory pool\n",
    "\n",
    "Once GPU memory is allocated to an object, CuPy will expand it's *memory pool* by at least that amount. When an object is deallocated, the memory pool will hold on to that space in memory for faster subsequent operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ac79a8-60f6-41dd-96ee-ba09891ac5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mempool = cp.get_default_memory_pool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580ac30b-c4d8-458d-8b38-85711e851571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cupy_mem_stats():\n",
    "    print(f'GPU: used memory = {mempool.used_bytes() / 1024**2:.2f} MiB')\n",
    "    print(f'GPU: total pool  = {mempool.total_bytes() / 1024**2:.2f} MiB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd07a83-3bba-4f2d-aea3-dc8a6856bdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cupy_mem_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4352fca7-72ac-4724-8d86-4f7923bac772",
   "metadata": {},
   "source": [
    "Let's release our data for garbage collection. This will free up space in CuPy's memory pool on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5585921-d3fc-4627-8082-18861be3881f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del cpu_arr, gpu_arr, gpu_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449b2f11-983a-474f-a4bc-fcb7b6eb5a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "cupy_mem_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b087252-0fe0-49c9-a286-09bf30afef64",
   "metadata": {},
   "source": [
    "We can also explicitly release all GPU memory from CuPy's pool. This step can be useful if utilizing other GPU libraries in your workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56898727-8da5-4283-9bf2-6e3446812215",
   "metadata": {},
   "outputs": [],
   "source": [
    "mempool.free_all_blocks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e14be9-08e4-4c88-831b-beb523626abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cupy_mem_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f549db0-c793-4e73-a806-d774687b7bda",
   "metadata": {},
   "source": [
    "#### Let's track performance and memory usage for something bigger ...\n",
    "\n",
    "Rather than load a big data file, we will use the following function to produce a facimile temperature dataset. Feel free to play with the array size but be aware of the memory footprint!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774e9246-15f6-4a63-8819-597287129007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aqua_gen(nx, ny, nt):\n",
    "    \"\"\"Takes dimensions nx, ny, nt and returns an ndarray with facimile dataset.\"\"\"\n",
    "    \n",
    "    t = np.random.random((nx, ny, nt))\n",
    "    t = t + np.linspace(287, 289, nt)\n",
    "    t = t + np.sin(np.linspace(0.0, np.pi * nt, nt))\n",
    "    t = t + (np.sin(np.linspace(0.0, np.pi, ny)) * 20 - 10)[None,:,None]\n",
    "    \n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb35767-ec20-42b4-a825-aa4d8a000b7e",
   "metadata": {},
   "source": [
    "First, we will use the function to create a NumPy array of decent size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db53a903-1088-4658-a5d4-fb27639ab02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = aqua_gen(4000, 2000, 140)\n",
    "print(f'Size: {t.nbytes / 1024**3:.2f} GiB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a02945f-0bab-450d-ac7a-16ddf2d9cb72",
   "metadata": {},
   "source": [
    "We've produced a dataset that looks something like surface temperatures across an aquaplanet. (*at least one without weather!*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c257cfe-f3ce-4f19-b024-a02217b44142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.contourf(t[:,:,0].transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e758c59-85a4-4213-81ad-4860dfd06d61",
   "metadata": {},
   "source": [
    "Now we can copy it to the GPU and track what happens with the memory usage and pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ecece0-a0d8-4ae3-b5c7-d09dc951d456",
   "metadata": {},
   "outputs": [],
   "source": [
    "cupy_mem_stats()\n",
    "tg = cp.asarray(t)\n",
    "cupy_mem_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3080b7cb-1a92-4590-b516-2e4f51cf0001",
   "metadata": {},
   "source": [
    "Finally, let's compare the performance of an operation to convert to Celsius and take the time-mean at each grid point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e8b663-b3cf-4a96-88ae-52639c07d491",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time cpu_tm = np.mean((t - 273.15), axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ae09f1-b474-4a2d-a97e-b7822f8af06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time gpu_tm = cp.mean((tg - 273.15), axis = 2)\n",
    "cupy_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a4043b-13de-4e2f-9869-2680207868a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "del cpu_tm, gpu_tm, tg\n",
    "mempool.free_all_blocks()\n",
    "cupy_mem_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eb2a0a-1752-4887-a158-385d8c87a3ce",
   "metadata": {},
   "source": [
    "**GPU memory is smaller than node memory - if we need to process very large arrays of data, we will need a tool to parallelize across multiple GPUs ...**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1a1aed-3090-46d6-946a-e4ce5a4aaece",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2fa115-4a27-4788-9935-c3d9b1f55e35",
   "metadata": {},
   "source": [
    "## Using Dask with GPUs\n",
    "\n",
    "Dask is a popular Python library for parallelizing computations across multiple workers. It provides incredibly easily scaling of code from problem sizes that fit on a dual-core laptop to those that can span an entire analysis cluster like Casper.\n",
    "\n",
    "* Dask is easy to install\n",
    "* Dask leverages high-level collections like NumPy arrays and pandas DataFrames\n",
    "* Dask can schedule workers itself or submit jobs to batch schedulers like PBS and Slurm\n",
    "* Dask workers can use GPUs\n",
    "\n",
    "**Dask allows you to scale up from problem sizes that fit in node *or GPU* memory to ones that fit on disk.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f59c010-5446-40c3-af50-e139c7ea38c9",
   "metadata": {},
   "source": [
    "### Dask fundamentals and how the GPU is supported\n",
    "\n",
    "Dask operates on either a high-level collection of data like an array, Series, or DataFrame, or a low-level collection of arbitrary code marked for either `delayed` or asynchronous (`futures`) parallel execution. In this tutorial, we will focus on high-level data structures.\n",
    "\n",
    "| Dask collection | CPU object | GPU object | GPUs |\n",
    "|-|-|-|-|\n",
    "| dask.array | numpy.array | cupy.array | NVIDIA, AMD |\n",
    "| dask.series | pandas.series | cudf.series | NVIDIA |\n",
    "| dask.dataframe | pandas.dataframe | cudf.dataframe | NVIDIA |\n",
    "\n",
    "Most Dask operations use *lazy evaluation*, meaning you tell Dask what you want the workers to do, but then they wait until you start the computation using `.compute()` (or similar methods). These instructions are collectively called a *task graph*, and then map out what Dask will do to get from your data collection to your desired output. Notably, worker memory is not consumed until you begin executing the task graph.  \n",
    "</br>\n",
    "<div style=\"text-align:center;\"><img src=images/dask-overview.svg width=800px alt=\"Dask Flow\"></img></br><i>Image credit: Anaconda, Inc. and contributors</i></div>  \n",
    "</br>  \n",
    "\n",
    "**Fundamentally, nothing about this workflow is different when using GPUs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a810e4-3e3a-4817-8d86-c4a18942377c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### When might you use Dask with GPUs?\n",
    "\n",
    "While Dask is easy to use, even the simplest of parallel computing approaches can be tricky to get right. Here are some scenarios where scaling GPU resources with Dask can be powerful:\n",
    "\n",
    "1. You are lucky enough to have many GPU resources with low contention and can be bursty\n",
    "2. Your workflow benefits tremendously from GPU acceleration, but is too big for a single GPU\n",
    "3. A small segment of your analysis workflow can benefit from on-demand access of GPUs\n",
    "4. You are running a large application with MPI and wish to do between-cycle data work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59906c1d-9d2e-4112-85f1-10d7d9ab9190",
   "metadata": {},
   "source": [
    "#### Notable Dask packages\n",
    "\n",
    "Dask functions are broken into a diverse ecosystem of packages. Here is a list of notables for GPU usage:\n",
    "\n",
    "* **dask** - the core API, including collections and a basic task scheduler\n",
    "* **distributed** - cluster objects (groups of workers) that can span a network and detailed dashboards\n",
    "* **dask-jobqueue** - Dask HPC clusters (e.g., `PBSCluster`, `SLURMCluster`)\n",
    "* **dask-cuda** - tools to set up GPU-backed workers (especially on single nodes)\n",
    "* **dask-mpi** - construct clusters of workers using MPI processes (*can be used with dask-cuda*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ed2699-e18f-494c-ab0a-7e17980e622f",
   "metadata": {},
   "source": [
    "### Using NVIDIA's LocalCUDACluster\n",
    "\n",
    "The `LocalCUDACluster` cluster object comes from the **dask-cuda** package. It isn't required to use **CuPy** and **cuDF** collections with Dask, but it eases pinning local GPUs to specific workers and provides functionality for GPU memory spilling and high-speed worker communication via UCX. It is developed as part of RAPIDS.  \n",
    "\n",
    "*UCX requires [compilation and customization](https://docs.rapids.ai/api/dask-cuda/stable/ucx.html) so we will avoid here for the sake of simplicity.*\n",
    "\n",
    "#### First, we need to start our cluster...\n",
    "\n",
    "The **dask-cuda** [API document](https://docs.rapids.ai/api/dask-cuda/stable/api.html#cluster) is useful here for cluster settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b7944c-8ea6-4a31-846b-817b31eee5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask.distributed import Client\n",
    "from dask_cuda import LocalCUDACluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9785645f-4e95-4dde-bf7e-927e39ccae1e",
   "metadata": {},
   "source": [
    "By default, it would spawn `len(CUDA_VISIBLE_DEVICES)` workers.  \n",
    "\n",
    "We will also associate our cluster object with a `Client`. The client is our runtime interface to the cluster and Dask scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae084c2b-b53e-4b77-a05e-623677d42577",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCUDACluster()\n",
    "client = Client(cluster, timeout = '60s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c0e897-5e37-4ecd-b14b-8d30a494ba78",
   "metadata": {},
   "source": [
    "*Did it work?*\n",
    "\n",
    "If you are using multiple GPUs, the `CUDA_VISIBLE_DEVICES` environment variable needs to be set to integer device IDs, not string GPU UUIDs as PBS specifies.\n",
    "\n",
    "First, let's shut down our prior attempt to free the port for a new cluster and client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b2d111-433f-40d9-bbc6-b03434629ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9398f23b-7111-4338-a143-a75e92a37457",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Querying GPU information with **pynvml**\n",
    "\n",
    "The **pynvml** provides Python bindings to the NVIDIA Management Library (NVML), which allows users to query and (often with root) modify GPU characteristics and behavior. We can import and use it to get information about our GPUs from within Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e340ec3-1da8-4dcd-9150-d0d188b5935f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "nvmlInit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1967cc8-e679-480f-80ab-c8bb98a7d350",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmode_names = ['Default', 'Exclusive Thread', 'Prohibited', 'Exclusive Process']\n",
    "\n",
    "for i in range(nvmlDeviceGetCount()):\n",
    "    handle = nvmlDeviceGetHandleByIndex(i)\n",
    "    uuid = nvmlDeviceGetUUID(handle).decode(\"utf-8\")\n",
    "    cmode = nvmlDeviceGetComputeMode(handle)\n",
    "    print(f'Device {i} UUID: {uuid}; Compute mode: {cmode_names[cmode]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ca798b-ca93-4a29-a808-2c209f333160",
   "metadata": {},
   "source": [
    "**Heads up:** You are likely to run into issues in Python if you use a GPU that is not using the Default compute mode. Exclusive process mode, for example, will prevent you from using **CuPy** and a **CUDAWorker** in the same Python process!\n",
    "```\n",
    "Device 0 UUID: GPU-c0bdd08e-994c-f92d-6ad7-7b629156e76d; Compute mode: Exclusive Process\n",
    "Device 1 UUID: GPU-e8571304-3a7b-443e-d688-933be18cfbeb; Compute mode: Default\n",
    "```\n",
    "If you see something like the above, make sure to change (or more likely have your sys admin change) the compute mode to default either in Python or using `nvidia-smi`:\n",
    "```bash\n",
    "sudo nvidia-smi -c 0 -i UUID\n",
    "```\n",
    "#### Let's put CUDA_VISIBLE_DEVICES in a form (device IDs) that `LocalCUDACluster` will accept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7ceffb-9a31-490e-9313-b0e462d93522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd528332-b7dd-461e-ada4-db76c6e4d5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCUDACluster()\n",
    "client = Client(cluster, timeout = '60s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c4215b-8cb2-4bd3-b37a-7d86384cec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008d41a1-8259-4e02-a038-71703dd49f3b",
   "metadata": {},
   "source": [
    "**Now we are ready to scale up computations with Dask!**  \n",
    "\n",
    "We can use our aquaplanet temperature array we created earlier. Let's imagine that the array is a single output file, and we need to process twelve of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba561b3-adb1-4043-b728-b33d5b226bbf",
   "metadata": {},
   "source": [
    "#### Construct a Dask Array *on the GPU* using the NumPy array we created ...\n",
    "\n",
    "First we use the NumPy array to define our chunk size - the number of elements for each chunk of work to be done by a worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe63a43-dcf6-43db-91bb-562a97c4d38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_arr = da.from_array(t, chunks = (1000, 500, 140))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d22524-975f-4f5c-97ac-e29f90b69a7c",
   "metadata": {},
   "source": [
    "To create a Dask Array on the GPU, we simply need to map a conversion from NumPy to CuPy to all of our data chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe9cb35-ee39-4365-b2e8-391d83f803c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_arr = dc_arr.map_blocks(cp.asarray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da515cf9-2587-4124-bb79-babf3a90154b",
   "metadata": {},
   "source": [
    "We can now concatenate multiple copies of this dask array together to form our \"multifile\" dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec26955-04d3-445f-812b-046895fbc856",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_mf = da.concatenate([dg_arr] * 12, axis=2)\n",
    "dg_mf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce3f36c-cd6e-43d1-a895-634c6249a7dd",
   "metadata": {},
   "source": [
    "#### Assigning tasks to a Dask collection\n",
    "\n",
    "Let's do some data processing on our GPU Dask Array. We can convert from Kelvin to Celsius and then take the mean across all times at each spatial index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a5bf05-0d1f-4afa-bb1c-55cd79de3af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tg = da.mean((dg_mf - 273.15), axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f0e0eb-30f7-43f4-8335-88b5dac35c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tg.dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761d3d8e-fd4c-40c0-bdbb-0f9365a44368",
   "metadata": {},
   "source": [
    "Using the above attribute, we can see the tasks that Dask will run, but remember, it does not actually perform any computation (*or even map our CPU array into GPU memory*) until we run `.compute()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46796830-8354-424e-8e30-f57cec6f659b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dg_means = tg.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51f1d0c-c4dd-4371-ab7b-20549d893d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del dg_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e248797-8a69-4a03-9014-15df7e58c2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9cddd5-ca08-427d-9586-c0cf956e66d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "del client, cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e5eb40-46b9-4e6c-bc8b-5d9d7ffc1ecd",
   "metadata": {},
   "source": [
    "### Scheduling GPUs with `dask-jobqueue`\n",
    "\n",
    "In this workflow, we use a scheduler cluster object like `PBSCluster` to submit batch jobs to run single workers. This cluster is often easier to augment with GPUs, as we only need to make a few tweaks to our worker resource specification and GPU pinning to each worker is done implicitly through job GPU assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9917c09-1f2d-4118-88b2-e2af18331bcc",
   "metadata": {},
   "source": [
    "#### Consider the following distributed CPU code\n",
    "\n",
    "```python\n",
    "from dask_jobqueue import PBSCluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "cluster = PBSCluster(\n",
    "    cores=1,\n",
    "    memory='35GiB',\n",
    "    processes=1,\n",
    "    resource_spec='select=1:ncpus=1:mem=35GB',\n",
    "    queue='casper',\n",
    "    walltime='10:00',\n",
    "    interface='ib0'\n",
    ")\n",
    "```\n",
    "\n",
    "#### Let's modify it to create GPU workers from dask_jobqueue import PBSCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80916dc5-0d5b-43fa-9e85-689d318aa00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_jobqueue import PBSCluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "cluster = PBSCluster(\n",
    "    cores=1,\n",
    "    memory='35GiB',\n",
    "    processes=1,\n",
    "    resource_spec='select=1:ncpus=1:mem=35GB:ngpus=1',\n",
    "    job_extra=['-l gpu_type=a100'],\n",
    "    queue='preview',\n",
    "    walltime='10:00',\n",
    "    interface='ib0'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f4256c-f0dc-464c-ad51-aa97f3733b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cluster.job_script())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a615d8a-3d22-4280-bc55-436a4f9230ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster, timeout = '60s')\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034479b7-62c4-4918-b56c-67c06930be4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster.scale(2)\n",
    "client.wait_for_workers(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad8b3be-270e-4a64-91b2-72b56100ed32",
   "metadata": {},
   "outputs": [],
   "source": [
    "!qstat -u $USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e248ee5-5e54-4257-b8cb-cb0d88f7027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dg_means = tg.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae51aba0-d3eb-4452-8bf2-8c1e759c76fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa46167-f34a-45b7-a6bb-74ef225bdca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!qstat -u $USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea9da49-4bd7-4334-ba8f-34bd062226aa",
   "metadata": {},
   "source": [
    "#### dask-jobqueue and dask-cuda compatibility\n",
    "You cannot launch CUDAWorkers via the *dask-jobqueue* Python API at this time, and so the interface for GPU memory spill is not accessible using this method. It may be possible to start the workers via [dask-cuda-worker](https://docs.rapids.ai/api/dask-cuda/stable/api.html#worker) from the shell on each host, and then connect your Client to the IP of a [command-line scheduler](https://docs.dask.org/en/stable/deploying-cli.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff535989-2d76-4147-b8a4-3b7e533d72f7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Some caveats to using Dask (especially with GPUs)\n",
    "\n",
    "Even in CPU computing, use of Dask should be carefully considered. While typically minimal, starting and managing Dask workers does incur overhead that can become large when using very high worker counts. It is also more challenging to debug Dask workflows compared to serial ones!\n",
    "\n",
    "**On GPUs:**\n",
    "* Setting up Dask is more difficult\n",
    "* Typical task sizes may starve the GPU for work\n",
    "* Worker memory management is even more important\n",
    "\n",
    "In some ways, Dask and GPU-offload typically solve orthogonal problems:\n",
    "\n",
    "| CuPy, cuDF, ... | Dask |\n",
    "|-----------------|------|\n",
    "| Lower time to solution | Bigger problem size |\n",
    "\n",
    "Of course Dask can be used to lower the time to solution as well, but often you are better off optimizing your serial code instead. (*and using a GPU may be a great choice if your problem is highly vectorizable!*)\n",
    "\n",
    "A big issue is that limited GPU availability (or higher cost) on many platforms may hamper Dask's power of scalability. On *Casper*, wait times for V100 GPUs are typically longer than for CPU resources. And GPUs feature much lower memory than many data-analysis CPU nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78babce5-c0bf-43b5-9b7b-7a79bdec591e",
   "metadata": {},
   "source": [
    "## **cuDF**: Series and DataFrames on GPUs\n",
    "\n",
    "A *DataFrame* object is simply an annotated 2D table of columnar data. In Python, the `pandas` library is the go-to package for working with these object representations of spreadsheets, and their simpler single-column variant, the *series*.\n",
    "\n",
    "#### Let's download some data\n",
    "\n",
    "The following dataset contains daily records of Boulder, CO weather from 1897 to present-day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a186b3f4-087f-4912-8040-dd975f3c0d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://psl.noaa.gov/boulder/data/boulderdaily.complete.txt | grep \"^ [12]\" > data/daily.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49635d3e-ff41-47c7-9b48-3a87b525a7b0",
   "metadata": {},
   "source": [
    "#### We can quickly read in and inspect the data on the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d258191-9a27-4bec-ab24-a56acfca98a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36375ec1-9ad2-49c2-8d93-67dc2a6679bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "col_names = ['year', 'month', 'day', 'tmax', 'tmin', 'precip', 'snow', 'snowcover']\n",
    "cpu_df = pd.read_csv('data/daily.txt', delim_whitespace=True, names = col_names,\n",
    "                     na_values = [-998, -999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658c44ca-bf68-4e62-a221-d5db77c2300b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3d2227-0bff-4819-8a2d-50249616d534",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Mem used: {cpu_df.memory_usage(deep = True).sum() / 1024**2:.2f} MiB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eb61e0-cbb1-4592-b7a7-32929b9c80d7",
   "metadata": {},
   "source": [
    "*Note: `memory_usage` returns a pandas series here. We take the sum to get the amount used over all columns and specify `deep = True` to account for mutable data referenced within the DataFrame.*\n",
    "\n",
    "#### Average snowfall in each month\n",
    "\n",
    "**Pandas** can succinctly and easily compute complex statistics on DataFrames and then plot them with **matplotlib**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2e4179-648f-439d-8409-a02eea831ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "avg_snow = cpu_df[cpu_df.year > 1897].groupby(['year', 'month']).sum().groupby('month').snow.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e23696-10c4-4850-8529-10fa8e8d2ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_snow.plot.area(ylabel = \"Average Snowfall [in]\", xlim = (1,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5e47fb-43b1-4a7c-a1cf-f20d16721c86",
   "metadata": {},
   "source": [
    "#### Replacing **pandas** with **cuDF** for GPU use\n",
    "\n",
    "For *series* and *DataFrames*, **cuDF** allows us to leverage GPUs for computation. Note that unlike **CuPy**, the **cuDF** package is part of RAPIDS and developed by NVIDIA. Thus it will NOT work on other hardware at this time.\n",
    "\n",
    "Many **pandas** methods and functions and have been replicated with **cuDF**, allowing for near drop-in replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d5edab-1ba4-4442-94cf-54173e58c773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf as cd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedf05b2-b201-4c80-8072-e32d483c5b49",
   "metadata": {},
   "source": [
    "*This doesn't look the same, however!*\n",
    "\n",
    "It turns out we need to make a few modifications for cuDF to read the data in:\n",
    "1. cuDF does not ignore a leading space on each row, so we handle this by assigning the column a dummy name and then dropping it after it is read.\n",
    "2. cuDF will not take integers for the na values, and so we use `.replace` after read-in to get the same effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e76aa8-b1fc-4b99-8e41-c94567d1f810",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gpu_df = cd.read_csv('data/daily.txt', delim_whitespace = True,\n",
    "                     names = ['x'] + col_names).drop(columns = 'x').replace([-998, -999], cd.NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d625a68a-d8fd-42f8-b252-a795e672234f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea07ffa5-e777-418b-b8e3-aa62dadd68e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Mem used: {gpu_df.memory_usage(deep = True).sum() / 1024**2:.2f} MiB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de81f593-9c7a-476f-9021-7454aac93d0e",
   "metadata": {},
   "source": [
    "Here again we must make a small modification to our CPU code - cuDF's groupby operation will not operate on a *column-index* unless it is explicitly told to do so with the `level` parameter. pandas will infer that we mean the index by names, but cuDF better adheres to pandas' own specification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca9b613-4e43-4a32-9f7f-f7d2ff288e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gpu_snow = gpu_df[gpu_df.year > 1897].groupby(['year', 'month']).sum().groupby(level = 'month').snow.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a2b1fa-e777-4102-b3b3-21c1a18b0ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_snow.to_pandas().plot.area(ylabel = \"Average Snowfall [in]\", xlim = (1,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bd2995-94fd-499d-a53e-193d03882732",
   "metadata": {},
   "source": [
    "That's an interesting plot. :-) It brings us to another difference between pandas and cuDF - it's groupby operations do not guarantee order as pandas' do. As such, if we want to get the pandas-equivalent result, we can sort the resulting data series by its index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af853bdd-6b22-4a1c-870e-88ee7f0c0475",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_snow.to_pandas().sort_index().plot.area(ylabel = \"Average Snowfall [in]\", xlim = (1,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b4eefc-4a4e-4e3b-9cdc-4c8da18eb3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "del cpu_df, gpu_df, avg_snow, cpu_snow, gpu_snow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf4e65e-606c-4e8d-b157-2512f6a0a6c4",
   "metadata": {},
   "source": [
    "#### **cuDF** benefits appear at scale\n",
    "\n",
    "Last time, we saw that **CuPy** only sped up calculations compared to **Numpy** at a certain problem size. We can show the same phenomenon with *DataFrames*.\n",
    "\n",
    "Here, we use a big [Earth Surface Temperature dataset](https://www.kaggle.com/datasets/berkeleyearth/climate-change-earth-surface-temperature-data) provided at Kaggle from Berkeley Earth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc18cbc-2fb6-4507-9916-0a87438b1f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh data/GlobalLandTemperaturesByCity.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e43eef8-4daa-459f-aadb-49ad8cec2892",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_big = pd.read_csv('data/GlobalLandTemperaturesByCity.csv',\n",
    "                      infer_datetime_format = True, parse_dates = [0])\n",
    "print(f'Mem used: {cpu_big.memory_usage(deep = True).sum() / 1024**3:.2f} GiB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0115fa-ef96-411c-b2bb-5270cae72ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gpu_big = cd.DataFrame.from_pandas(cpu_big)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deb65a4-08cd-495c-92ea-eadec027796a",
   "metadata": {},
   "source": [
    "**Compute global mean August temperature timeseries using the CPU and GPU ...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d42309b-465c-44a0-ad83-062501664fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "global_mean = cpu_big[(cpu_big.dt.dt.month == 8) &\n",
    "                      (cpu_big.dt.dt.year >= 1900)].groupby('dt')['AverageTemperature'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d63952-b310-44dc-8d9d-5e1118fdfa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "global_mean = gpu_big[(gpu_big.dt.dt.month == 8) &\n",
    "                      (gpu_big.dt.dt.year >= 1900)].groupby('dt')['AverageTemperature'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8024f7b-6726-49fa-9f4e-613a8ce5e33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_mean.to_pandas().sort_index().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec77cb1-6872-423c-a42c-264c022770ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "del cpu_big, gpu_big, global_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30311f1b-d792-4915-b011-6e67d301d2b7",
   "metadata": {},
   "source": [
    "#### **cuDF-pandas** compatibility\n",
    "\n",
    "Some notable differences ([more here](https://docs.rapids.ai/api/cudf/stable/user_guide/pandas-comparison.html)) between cuDF and Pandas:\n",
    "* `.join` and `.groupby` operations do not guarantee the row order of the output with **cuDF**\n",
    "* Iteration over **cuDF** objects is not allowed, as GPUs are not efficient for sequential tasks\n",
    "* Storing arbitrary Python objects (e.g., a column with strings and lists) is not allowed in **cuDF**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55b358b-6e25-44fb-93ee-480a2c8d40e5",
   "metadata": {},
   "source": [
    "### Xarray - N-dimensional labeled arrays\n",
    "\n",
    "Xarray can organize layered annotated sets of array data into data structures that enable complex operations on these N-dimensional datasets. It couples tightly with netCDF/Zarr data and Dask and is very popular at NCAR. As demonstrated last week, it is possible to use CuPy arrays as the underlying data structure in Xarray, enabling offload to GPUs. Efforts to improve compatibility and usability are ongoing - reports of any issues or needed functionalities are desired by the maintainers! (see [this GitHub repo](https://github.com/xarray-contrib/cupy-xarray))\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c9fa25-d281-4b3a-a591-bf27b776545b",
   "metadata": {},
   "source": [
    "## Dask Resources\n",
    "\n",
    "[Dask Online Tutorial](https://tutorial.dask.org/)\n",
    "\n",
    "[Dask Distributed Documentation](https://distributed.dask.org/en/stable/)\n",
    "\n",
    "[dask-jobqueue Documentation](https://jobqueue.dask.org/en/latest/)\n",
    "\n",
    "[dask-cuda Documentation](https://docs.rapids.ai/api/dask-cuda/stable/)\n",
    "\n",
    "[How to: Use Dask with GPUs](https://docs.dask.org/en/stable/gpu.html)\n",
    "\n",
    "[dask-mpi and GPUs](http://mpi.dask.org/en/latest/gpu.html)\n",
    "\n",
    "[Dask on GPUs Speedups](https://blog.dask.org/2019/01/03/dask-array-gpus-first-steps)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6c30ad-ae31-4178-895b-feefc090ea7c",
   "metadata": {},
   "source": [
    "\n",
    "## High-level Collections Resources\n",
    "\n",
    "[CuPy Documentation](https://docs.cupy.dev/en/stable/index.html)\n",
    "\n",
    "[cuDF Documentation](https://docs.rapids.ai/api/cudf/stable/)\n",
    "\n",
    "[cupy-xarray Quickstart](https://cupy-xarray.readthedocs.io/quickstart.html)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823b837c-eee1-49ba-86c3-e5743ed4edb5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Running this notebook yourself\n",
    "### Notebook Setup\n",
    "\n",
    "This notebook will require running within a JupyterHub PBS Batch (interactive) session.  Much of the Python code will fail if a GPU is not detected on the node.  Select the PBS Batch option when launching from JupyterHub and set the `PROJECT` code to a currently active project, ie `UCIS0004` for the GPU workshop, and `QUEUE` to the appropriate routing queue depending on if during weekday 8am to 5:30pm MT (`gpudev`), or all other times (`casper`). Request `30` GB of memory for this session. Due to limited shared GPU resources, please use `GPU_TYPE=gp100` during the workshop. Otherwise, set `GPU_TYPE=v100` (required for `gpudev`) for independent work. See [Casper queue documentation](https://arc.ucar.edu/knowledge_base/72581396#StartingCasperjobswithPBS-Concurrentresourcelimits) for more info.\n",
    "\n",
    "If running this notebook outside of the NCAR computing environment, at least one compatible NVIDIA GPU is required to run most of the local cuDF and Dask examples - and access to a batch cluster with GPUs is required for the Dask Distributed examples.\n",
    "\n",
    "### Changing Notebook Kernel\n",
    "\n",
    "You'll need to use a JupyterHub kernel that features the RAPIDS suite of packages (at least `CuPy`, `cuDF`, and various `Dask` packages).  There are several ways to do this but you can navigate to the `Kernel` dropdown and select `Change Kernel`.  Select **GPU Workshop** kernel from the dropdown to run the CuPy examples in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e639a5f1-9551-41af-b4ec-29836da8ee9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Python Virtual Environment Setup\n",
    "\n",
    "You can set up your own virtual environment for running the Python code outside of this notebook.  This will also be useful if you would like to create your own virtual environment for GPU programming experimentation with Python.\n",
    "\n",
    "See [Python Conda environment documentation](https://kb.ucar.edu/display/RC/Using+conda+environments+for+Python+access) for background on using Conda on NCAR clusters. Use conda (or mamba for faster package solving) to create your own GPU-enabled environment. Here are the steps for doing so on Casper:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd9c63f-414a-4511-943f-975b8b6aa977",
   "metadata": {
    "tags": []
   },
   "source": [
    "```bash\n",
    "# Conda used for virtual environment in NCAR clusters\n",
    "module load conda/latest\n",
    "\n",
    "# Create the environment and the populate it with packages\n",
    "# Note that we need channel priority flexible for RAPIDS to install\n",
    "conda create -n pygpu-dask\n",
    "conda activate pygpu-dask\n",
    "conda config --env --set channel_priority flexible\n",
    "conda env update --file envs/environment.yml\n",
    "\n",
    "# Don't forget to deactivate with \"conda deactivate\"\n",
    "\n",
    "```\n",
    "\n",
    "If running on other systems, you should install Jupyter as well so that you may use notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pygpu-dask]",
   "language": "python",
   "name": "conda-env-pygpu-dask-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
