{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3954377-3b44-46f7-97e1-33cd3b9fd529",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "![NCAR UCAR Logo](NCAR_CISL_NSF_banner.jpeg)\n",
    "# Multi-GPU Programming, Part 2\n",
    "\n",
    "By: Daniel Howard [dhoward@ucar.edu](mailto:dhoward@ucar.edu), Consulting Services Group, CISL & NCAR \n",
    "\n",
    "Date: July 14th, 2022\n",
    "\n",
    "In this notebook we explore the mini-app [MiniWeather](https://github.com/mrnorman/miniWeather) to present techniques and code examples for implementing and assessing performance of various multi-GPU paradigms. We will cover:\n",
    "\n",
    "1. Interoperability of OpenACC with MPI and NCCL GPU communication libraries\n",
    "2. Hands-on implementation of MiniWeather with CUDA aware MPI and NCCL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a041b470-f95d-4a39-816a-b5f6792dbcba",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Head to the [NCAR JupyterHub portal](https://jupyterhub.hpc.ucar.edu/stable) and __start a JupyterHub session on Casper login__ (or batch nodes using 1 CPU, no GPUs) and open the notebook at `11_MultiGPU/11_multiGPU_Part2.ipynb`. Be sure to clone (if needed) and update/pull the NCAR GPU_workshop directory.\n",
    "\n",
    "```shell\n",
    "# Use the JupyterHub GitHub GUI on the left panel or the below shell commands\n",
    "git clone git@github.com:NCAR/GPU_workshop.git\n",
    "git pull\n",
    "```\n",
    "\n",
    "# Workshop Etiquette\n",
    "* Please mute yourself and turn off video during the session.\n",
    "* Questions may be submitted in the chat and will be answered when appropriate. You may also raise your hand, unmute, and ask questions during Q&A at the end of the presentation.\n",
    "* By participating, you are agreeing to [UCAR’s Code of Conduct](https://www.ucar.edu/who-we-are/ethics-integrity/codes-conduct/participants)\n",
    "* Recordings & other material will be archived & shared publicly.\n",
    "* Feel free to follow up with the GPU workshop team via Slack or submit support requests to [rchelp.ucar.edu](https://support.ucar.edu)\n",
    "    * Office Hours: Asynchronous support via [Slack](https://ncargpuusers.slack.com) or schedule a time with an organizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b64df7-2c23-4048-af67-1590fdeb1b79",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Notebook Setup\n",
    "Set the `PROJECT` code to a currently active project, ie `UCIS0004` for the GPU workshop, and `QUEUE` to the appropriate routing queue depending on if during a live workshop session (`gpuworkshop`), during weekday 8am to 5:30pm MT (`gpudev`), or all other times (`casper`).\n",
    "\n",
    "__The`GPU_TYPE=gp100` nodes are not configured for multi-GPU computing!__ Thus, the `gpuworkshop` queue is not useful for this session. Saying as much, please set `GPU_TYPE=v100` and use the `gpudev` both during the workshop and for independent work. See [Casper queue documentation](https://arc.ucar.edu/knowledge_base/72581396#StartingCasperjobswithPBS-Concurrentresourcelimits) for more info.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81bda30-dee0-48f9-927e-970e9baae6f9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "export PROJECT=UCIS0004\n",
    "export QUEUE=casper\n",
    "export GPU_TYPE=v100\n",
    "\n",
    "module load cuda/11.6 nvhpc/22.5 openmpi/4.1.4 &> /dev/null\n",
    "export PNETCDF_INC=/glade/u/apps/dav/opt/pnetcdf/1.12.3/openmpi/4.1.4/nvhpc/22.5/include\n",
    "export PNETCDF_LIB=/glade/u/apps/dav/opt/pnetcdf/1.12.3/openmpi/4.1.4/nvhpc/22.5/lib\n",
    "export NCCL_INC=/glade/u/apps/opt/nvhpc/22.5/Linux_x86_64/22.5/comm_libs/nccl/include\n",
    "export NCCL_LIB=/glade/u/apps/opt/nvhpc/22.5/Linux_x86_64/22.5/comm_libs/nccl/lib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15225a03-f1ee-403f-a0cb-779170b0db56",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## What Have We Learned So Far?\n",
    "\n",
    "In [Part 1](Multi-GPU_Programming_for_Earth_Scientists_Jiri_Kraus_NVIDIA.pdf), Jiri Kraus from NVIDIA shared many different approaches for Multi-GPU Programming.\n",
    "\n",
    "* Non-CUDA Aware MPI\n",
    "* CUDA Aware MPI\n",
    "* NCCL (pronounced \"nickel\") - NVIDIA Collective Communication Library\n",
    "* NVSHMEM - NVIDIA Shared Memory Library\n",
    "* Other tips and backgrond information\n",
    "\n",
    "Today, we will focus on hands-on implementation of __CUDA Aware MPI__ and __NCCL__ within MiniWeather."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5770e7d5-2d04-4705-a4ff-aa10ec9ec938",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Baseline Performance of Non-CUDA Aware MPI\n",
    "First, let's compile our baseline program `miniWeather_mpi_openacc.F90`. To note, __I/O output has been disabled in all versions__. Also, this version has already been partially modified to offload each MPI task to a distinct GPU.\n",
    "\n",
    "Casper does not attempt to isolate GPUs between MPI tasks like other HPC centers may choose to do by default. On Casper, __every MPI task can access all GPUs available to the node__ it is residing on.\n",
    "\n",
    "__Question__: If this baseline code was not modified, why does MiniWeather's performance not improve with increasing the number of MPI tasks that reside on the same node? \n",
    "\n",
    "__Hint__: Which GPU device is each MPI task using?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1d2a41-eceb-483f-a0ba-aadb122b8a6a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compiles the CUDA aware MPI version of MiniWeather using OpenACC\n",
    "OPENACC_FLAGS=\"-acc -gpu=cc70,lineinfo\"\n",
    "\n",
    "mpif90 -I${PNETCDF_INC} -Mextend -O0 -DNO_INFORM -c miniWeather_mpi_openacc.F90 -o miniWeather_mpi_openacc.F90.o \\\n",
    "-D_NX=4096 -D_NZ=2048 -D_SIM_TIME=10.0 -D_OUT_FREQ=10.0 -D_DATA_SPEC=DATA_SPEC_THERMAL ${OPENACC_FLAGS}\n",
    "\n",
    "mpif90 -Mextend -O3 miniWeather_mpi_openacc.F90.o -o openacc -L${PNETCDF_LIB} -lpnetcdf ${OPENACC_FLAGS}\n",
    "rm -f miniWeather_mpi_openacc.F90.o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c468828-f687-4d29-ba4a-aeac38a3a09c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Now, submit multi-GPU runs to get some performance benchmarks. We use a validation script to ensure the answer is correct. The validation script has the following usage syntax, where `n_tasks` is optional:\n",
    "\n",
    "`./check_output.sh  executable  mass_relative_tolerance  energy_relative_tolerance  [n_tasks]`\n",
    "\n",
    "To note, Casper has nodes of 4 or 8 GPUs available and the main `gpgpu` queue allows up to 32 GPUs per job. The `gpudev` queue allows up to 4 GPUs per job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb7eb85-3480-4d3f-b4fd-97c79d695151",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for S in 1       # Select number of nodes (Casper: gpgpu -> S*N <= 32, gpudev -> S*N <= 4 )\n",
    "  do\n",
    "  for N in 1 2 4   # Number of MPI tasks, CPUs, and GPUs per node (Casper: max(N)=8, 8-way nodes often busy )\n",
    "    do\n",
    "      qcmd -A $PROJECT -q $QUEUE -l select=$S:ncpus=$N:ngpus=$N:mpiprocs=$N -l gpu_type=$GPU_TYPE -l walltime=30 -- \\\n",
    "      $PWD/check_output.sh $PWD/openacc 1e-13 4.5e-5 $((S*N))\n",
    "    done\n",
    "  done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b594c51-619a-408f-9825-6a7c2b7d5fe7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Basics of Communication with MPI\n",
    "\n",
    "The foundation to communicating data and messages for a __distributed-memory__ computer is the __Message Passing Interface (MPI)__ library. MPI consists of a collection of routines for exchanging data across distributed memory spaces in a parallel program, ie memory from one node to another node or one GPU to another GPU.\n",
    "\n",
    "![MPI for Two Nodes](img/MPI_2Nodes.jpeg)\n",
    "\n",
    "The MPI standard was first introduced in 1994 and has evolved many times over the years to a well established level of maturity across multiple library options, such as __OpenMPI__, __Intel MPI__, and __MVAPICH__.\n",
    "\n",
    "A complete description of the MPI standard can be found on the __[MPI Forum's Documentation Page](https://www.mpi-forum.org/docs/)__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feae2394-3dee-4fb8-bbfe-617e0a76e1c2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Basic Structure of MPI Programs\n",
    "\n",
    "1. Initialize communications\n",
    "    * `MPI_INIT` - initializes the MPI environment\n",
    "    * `MPI_COMM_SIZE` - returns the number of processes\n",
    "    * `MPI_COMM_RANK` - returns this process’s number (rank)\n",
    "2. Communicate to share data between processes\n",
    "    * `MPI_SEND` - sends a blocking message\n",
    "    * `MPI_RECV` - receives a blocking message\n",
    "3. Exit from the message-passing system --\n",
    "    * `MPI_FINALIZE`\n",
    "    \n",
    "There also exists collective operations such as `MPI_Bcast` and `MPI_Allreduce`. A primary concept to understand is __blocking__ vs __non-blocking__ communication, ie `MPI_Send` vs `MPI_ISend`.\n",
    "\n",
    "This concept is similar to __synchronous__ vs __asynchnronous__ operations, discussed in earlier GPU sessions respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce0db13-25af-4f0e-9213-235424b7dee9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### __Data Parameters__ - Example `MPI_Send` and `MPI_Recv`\n",
    "\n",
    "![MPI Data Parameters](img/MPIDataParameters.gif)\n",
    "\n",
    "Source: Cornell's [Message Passing Interface Virtual Workshop](https://cvw.cac.cornell.edu/MPI/messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc94fc8-4ed1-47bd-bd94-e632348293a5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### __Envelope Parameters__ - Example `MPI_Send` and `MPI_Recv`\n",
    "\n",
    "\n",
    "\n",
    "![MPI Data Parameters](img/MPIEnvelopeParameters.gif)\n",
    "\n",
    "Source: Cornell's [Message Passing Interface Virtual Workshop](https://cvw.cac.cornell.edu/MPI/messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d79c7db-c5a1-48b8-acc6-f494978a83c1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Extension of MPI to Multi-GPU Communication\n",
    "\n",
    "There is not time today to go into more details on MPI. Nonetheless, MPI is an important framework to understand and forms the basis of development for similar Multi-GPU communication patterns and developed libraries.\n",
    "\n",
    "For example, the __NCCL__ library utilizes the communication APIs `ncclSend` and `ncclRecv`, which are functionaly equivalent to `MPI_Send` and `MPI_Recv`. Thus, concepts for MPI communication are very useful for understanding related concepts in multi-GPU communication patterns. __The only caveat is the additional separate memory space within the GPU alongside the CPU memory__.\n",
    "\n",
    "If you are not that familiar with MPI or want to review beginner/advanced concepts, you are encouraged to seek out additional learning material such as:\n",
    "* Cornell's Virtual Workshop [5-part MPI Series](https://cvw.cac.cornell.edu/topics#MPI)\n",
    "* NCSA's and UIUC's [Introduction to MPI](https://www.hpc-training.org/xsede/moodle/enrol/index.php?id=34) on the [hpc-training.org](https://www.hpc-training.org/xsede/moodle/) HPC-Moodle platform\n",
    "* XSEDE's [HPC Workshop: MPI](https://www.psc.edu/resources/training/xsede-hpc-workshop-may-2022-mpi/) May 2022 offering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63f34dc-4259-460a-a1d9-3d1c4b42bf68",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Using CUDA Aware MPI for Multi-GPU Communication\n",
    "\n",
    "Fortunately, [MiniWeather](https://github.com/mrnorman/miniWeather) already provides an MPI implementation. With OpenACC, calls to MPI had to be surrounded by the directives `!$acc update host()` and `!$acc update device()` (Note: `MPI_ISend`/`MPI_IRecv` are non-blocking MPI calls).\n",
    "\n",
    "```fortran\n",
    "!Prepost receives\n",
    "    call mpi_irecv(recvbuf_l,hs*nz*NUM_VARS,MPI_REAL8, left_rank,0,MPI_COMM_WORLD,req_r(1),ierr)\n",
    "    call mpi_irecv(recvbuf_r,hs*nz*NUM_VARS,MPI_REAL8,right_rank,1,MPI_COMM_WORLD,req_r(2),ierr)\n",
    "\n",
    "    !OpeanACC GPU Kernel loading send buffers\n",
    "    !$acc update host(sendbuf_l,sendbuf_r) async\n",
    "    !$acc wait\n",
    "\n",
    "    !Fire off the sends\n",
    "    call mpi_isend(sendbuf_l,hs*nz*NUM_VARS,MPI_REAL8, left_rank,1,MPI_COMM_WORLD,req_s(1),ierr)\n",
    "    call mpi_isend(sendbuf_r,hs*nz*NUM_VARS,MPI_REAL8,right_rank,0,MPI_COMM_WORLD,req_s(2),ierr)\n",
    "    !Wait for receives to finish\n",
    "    call mpi_waitall(2,req_r,status,ierr)\n",
    "    ...\n",
    "```\n",
    "\n",
    "This approach works but is not most optimal given available communication routes between nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3231dd-0a9a-4ef8-8b82-a4fd3c2097a2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Recall from the previous session how data must be copied from GPU memory to CPU memory to another node's CPU memory to that node's GPU memory.\n",
    "\n",
    "![MPI Memory Movement without CUDA Aware](img/MPI_MemoryMovement_NonAware.png)\n",
    "\n",
    "Some of these memory movement steps can be eliminated with better programming choices. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d92780-42a4-47e4-b4cb-6b2d53c0ce1c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### OpenACC Interoperability and Implemenations for CUDA Aware MPI\n",
    "\n",
    "In order to redirect memory movement and avoid unecessary steps between the CPUs and GPUs, __MPI must be provided the GPU device location of data memory__. Here's an example in OpenACC:\n",
    "\n",
    "```fortran\n",
    "!$acc host_data use_device(s_buf_d,r_buf_d)\n",
    "...\n",
    "!$acc end host_data\n",
    "```\n",
    "\n",
    "Essentialy, within any `host_data` on the CPU host, data objects in `use_device()` will no longer reference host/CPU memory but will instead point to device/GPU memory. This simple directive should tightly encapsulate any MPI calls, allowing a CUDA Aware MPI library to directly reference memory on the GPU instead of memory on the CPU. If interested, see more OpenACC interoperability features at [this GitHub](https://github.com/OpenACC/openacc-interoperability-examples) by Jeff Larkin.\n",
    "\n",
    "For Casper, the default library `OpenMPI` enables CUDA aware features by default. However, different MPI libraries often require you to specify additional flags or environment variables to use CUDA aware features (slide 22 in [Part 1](Multi-GPU_Programming_for_Earth_Scientists_Jiri_Kraus_NVIDIA.pdf))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94abaa9-fdbf-4a47-8d66-28666564dcb1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Once the MPI library directly references GPU device memory when setting up communication, messages communicated across a HPC cluster can more directly travel to their destinations, shortening the time spent in communication.\n",
    "\n",
    "![MPI Memory Movement with CUDA Aware](img/MPI_MemoryMovement_Aware.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9003ebdd-d7fd-4cd1-a274-f665c804b576",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Using OpenACC to Assign GPU Devices to MPI Tasks\n",
    "\n",
    "Parallel programs typically won't automatically know which GPU device it should be assigned to. Managing this is best left to the developer, which allows multiple arrangements, ie ...\n",
    "* one process per MPI task per GPU\n",
    "* one process managing multiple GPUs, etc.\n",
    "\n",
    "![GPU ranks across multiple nodes](img/Global_Local_GPURanks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ead064-1a23-437e-8674-385280ad2281",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "A common technique to manage local GPU ranks across global MPI ranks utilizes a split `MPI_COMM_TYPE_SHARED` communicator. This instantiates another MPI communicator that is local to an individual node, indexing a subset of processes across the local node's available MPI tasks.\n",
    "\n",
    "With a split communicator, you can then use OpenACC Runtime API functions like `acc_get_num_devices()` and `acc_set_device()` (must specify `use openacc`) to assign MPI tasks to specific GPUs given the devices available. An example initialization code snippet is below:\n",
    "\n",
    "```fortran\n",
    "    call MPI_Init(ierr)\n",
    "    call MPI_Comm_size(MPI_COMM_WORLD, nranks, ierr)\n",
    "    call MPI_Comm_rank(MPI_COMM_WORLD, myrank, ierr)\n",
    "    call MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, myrank, MPI_INFO_NULL, local_comm, ierr)  \n",
    "    call MPI_Comm_rank(local_comm, local_rank, ierr)\n",
    "    \n",
    "    nGPUs_node = acc_get_num_devices(acc_get_device_type())\n",
    "    call acc_set_device_num(mod(local_rank,nGPUs_node), acc_get_device_type())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0df234d-70fc-44aa-a590-abb27ba8590c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### EXERCISE - Implement CUDA Aware MPI in `miniWeather_mpiAware_openacc.F90`\n",
    "\n",
    "Following the direction of the `TODO` sections in [miniWeather_mpiAware_openacc.F90](miniWeather_mpiAware_openacc.F90), create a CUDA aware MPI version of MiniWeather. This will involve the following changes:\n",
    "\n",
    "1. [Line 78 & 80](miniWeather_mpiAware_openacc.F90#L78) - Instantiate additional variables for the split local communicators and number of GPUs\n",
    "2. [Line 401 & 423](miniWeather_mpiAware_openacc.F90#L401) - Specify the send and receive buffers in MPI calls to use GPU device memory\n",
    "3. [Line 505 & 525](miniWeather_mpiAware_openacc.F90#L505) - Add a split communicator and assign GPUs per ranks local to each node\n",
    "\n",
    "Once finished, use the cells below to compile the CUDA aware version, check for errors, and run it. If you get stuck, solutions are in the [solutions](solutions) folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec034f04-2352-405d-b701-8085f0796160",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compiles the CUDA aware MPI version of MiniWeather using OpenACC, exec = ./openaccAware\n",
    "OPENACC_FLAGS=\"-acc -gpu=cc70,lineinfo\"\n",
    "\n",
    "mpif90 -I${PNETCDF_INC} -Mextend -O0 -DNO_INFORM -c miniWeather_mpiAware_openacc.F90 -o miniWeather_mpiAware_openacc.F90.o \\\n",
    "-D_NX=4096 -D_NZ=2048 -D_SIM_TIME=10.0 -D_OUT_FREQ=10.0 -D_DATA_SPEC=DATA_SPEC_THERMAL ${OPENACC_FLAGS}\n",
    "\n",
    "mpif90 -Mextend -O3 miniWeather_mpiAware_openacc.F90.o -o openaccAware -L${PNETCDF_LIB} -lpnetcdf ${OPENACC_FLAGS}\n",
    "rm -f miniWeather_mpiAware_openacc.F90.o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb3f166-1098-43ca-a69a-5708d44a76b4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CUDA Aware MPI runs\n",
    "S=1; N=4\n",
    "  qcmd -A $PROJECT -q $QUEUE -l select=$S:ncpus=$N:ngpus=$N:mpiprocs=$N -l gpu_type=$GPU_TYPE -l walltime=30 -- \\\n",
    "  $PWD/check_output.sh $PWD/openaccAware 1e-13 4.5e-5 $((S*N))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308ddb16-53b7-4b7f-ae1a-b287efdc8b7d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Using NCCL Library for Multi-GPU Communication\n",
    "\n",
    "Leveraging the NVIDIA Collective Communication Library (NCCL) with OpenACC is relatively the same as CUDA Aware MPI. Below are some significant points to make:\n",
    "\n",
    "1. Function prototypes for calling NCCL only slightly differ from MPI functions\n",
    "2. By default, __all NCCL functions are blocking__ within the provided CUDA stream\n",
    "    * Grouped non-blocking behavior can be achieved with `ncclGroupStart()` and `ncclGroupEnd()` regions\n",
    "3. __NCCL can directly leverage scheduling and overalapping of communication/compute on the GPU__ through effective CUDA stream selection while MPI cannot\n",
    "4. Error handling is done via returned values from NCCL functions whereas MPI error handling is a passed argument\n",
    "5. __NCCL arguably should have better collective communication performance__ compared to MPI but point-to-point communication will likely not have demonstrable benefit\n",
    "    * NCCL can achieve improved performance on a well configured system since it automatically can detect and optimize communication across given __node topologies__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e05574-cab5-458d-9717-48ef2e87f936",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Using the NCCL API\n",
    "\n",
    "* Full documentation for NCCL is [here](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/index.html)\n",
    "* Listed functional prototypes and API specification is [here](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api.html) \n",
    "\n",
    "When using any external library, it is highly encouraged to consult available documentation like the above to learn how to use a library.\n",
    "\n",
    "![NCCL Gather Image](img/NCCL_Gather.png)\n",
    "\n",
    "In NCCL's case, the same `!$acc host_data use_device()` approach is required alongside OpenACC code (or you can directly pass in a device memory pointer from your own CUDA kernel or other library)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c146c098-42a2-493b-95fe-fae3d5392672",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Here are important data and function prototypes for NCCL useful for the next exercise:\n",
    "\n",
    "1. Data prototypes for initializing NCCL\n",
    "    * `type(ncclUniqueId) :: nccl_id` - A unique id for each group of communicators\n",
    "    * `type(ncclResult) :: nccl_result` - A variable to store return values from NCCL functions, used for error handling\n",
    "    * `type(ncclComm) :: nccl_comm` - The NCCL comunicator object\n",
    "2. Function prototypes for initializing NCCL\n",
    "    * `ncclResult_t ncclGetUniqueId(ncclUniqueId* uniqueId)` - Creates a `nccl_id` for the communicators\n",
    "    * `ncclResult_t ncclCommInitRank(ncclComm_t* comm, int nranks, ncclUniqueId commId, int rank)` - Initializes NCCL, similar to `MPI_Init()`\n",
    "3. Function prototypes for Send/Recv communication\n",
    "    * `ncclResult_t ncclSend(const void* sendbuff, size_t count, ncclDataType_t datatype, int peer, ncclComm_t comm, cudaStream_t stream)`\n",
    "    * `ncclResult_t ncclRecv(void* recvbuff, size_t count, ncclDataType_t datatype, int peer, ncclComm_t comm, cudaStream_t stream)`\n",
    "    \n",
    "To enable runtime NCCL debugging, simply set `NCCL_DEBUG`=`WARN`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4c296a-a83a-41af-8494-b2ad6bf6bbcb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### EXERCISE - Implement NCCL Library in `miniWeather_mpiNCCL_openacc.F90`\n",
    "\n",
    "Extending from the previous exercise, follow the `TODO` sections in [miniWeather_mpiNCCL_openacc.F90](miniWeather_mpiNCCL_openacc.F90) to create a NCCL version of Miniweather. Ideally, make this version portable by using the provided `#ifdef NV_GPU / #endif` preprocessor sections.\n",
    "\n",
    "1. [Line 14](miniWeather_mpiNCCL_openacc.F90#L14) - Load the NCCL library module\n",
    "2. [Line 93](miniWeather_mpiNCCL_openacc.F90#L93) - Instantiate NCCL variables needed for communicators\n",
    "3. [Line 425](miniWeather_mpiNCCL_openacc.F90#L425) - Setup a non-blocking group of sends and receives similar to the MPI sends and receives, also using OpenACC to reference GPU memory\n",
    "4. [Line 560](miniWeather_mpiNCCL_openacc.F90#L560) - Generate NCCL unique Id and use `MPI_Bcast` to broadcast to all ranks and initialize NCCL communicators\n",
    "\n",
    "Use the cells below to compile the NCCL version, check for errors, and run it.  If you get stuck, solutions are in the [solutions](solutions) folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4fce8d-462a-4ab1-b3c2-4b77abf15d88",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compiles the NCCL version of MiniWeather using OpenACC, exec = ./openaccNCCL\n",
    "OPENACC_FLAGS=\"-acc -gpu=cc70,lineinfo\"\n",
    "\n",
    "mpif90 -I${PNETCDF_INC} -I${NCCL_INC} -Mextend -O0 ${OPENACC_FLAGS} \\\n",
    "-DNV_GPU -DNO_INFORM -D_NX=4096 -D_NZ=2048 -D_SIM_TIME=10.0 -D_OUT_FREQ=10.0 -D_DATA_SPEC=DATA_SPEC_THERMAL \\\n",
    "-c miniWeather_mpiNCCL_openacc.F90 -o miniWeather_mpiNCCL_openacc.F90.o\n",
    "\n",
    "mpif90 -Mextend -O3 ${OPENACC_FLAGS} miniWeather_mpiNCCL_openacc.F90.o -o openaccNCCL \\\n",
    "-L${PNETCDF_LIB} -lpnetcdf -L${NCCL_LIB} -lnccl\n",
    "rm -f miniWeather_mpiNCCL_openacc.F90.o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a17564-e8c8-4853-b3f2-d71984ee42c5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NCCL runs\n",
    "S=1; N=4\n",
    "  qcmd -A $PROJECT -q $QUEUE -l select=$S:ncpus=$N:ngpus=$N:mpiprocs=$N -l gpu_type=$GPU_TYPE -l walltime=30 -- \\\n",
    "  $PWD/check_output.sh $PWD/openaccNCCL 1e-13 4.5e-5 $((S*N))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d11cba9-7592-4a0a-8173-9f7c3a9a0572",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Generate `nsys` Profiles of Multi-GPU Jobs\n",
    "Use the below cells and included [`nsysMPI_pbs.sh`](nsysMPI_pbs.sh) script to generate profile reports of each program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b8fe22-fa1d-4748-9fbb-9f82eedb0615",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qsub -q gpudev -v EXEC=openacc,N=4 -l select=1:ncpus=4:ngpus=4:mpiprocs=4 nsysMPI_pbs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0475eb62-b39b-48fc-b633-24e0ca96ee5b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qsub -q gpudev -v EXEC=openaccAware,N=4 -l select=1:ncpus=4:ngpus=4:mpiprocs=4 nsysMPI_pbs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4296d573-3137-4fc7-89da-3cf95319a4cd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qsub -q gpudev -v EXEC=openaccNCCL,N=4 -l select=1:ncpus=4:ngpus=4:mpiprocs=4 nsysMPI_pbs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126a0f82-a73a-4474-83b6-cbde01457e47",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Additional Considerations\n",
    "\n",
    "The following `UCX` and `OpenMPI` environment variables are currently recommended for optimal performance of CUDA Aware MPI applications. Future testing and system adjustements may modify these recommendations. Notably, `gdr_copy` is currently not incuded in `UCX_TLS`.\n",
    "\n",
    "```shell\n",
    "export CUDA_LAUNCH_BLOCKING=0\n",
    "export UCX_TLS=rc,sm,cuda_copy,cuda_ipc\n",
    "export OMPI_MCA_pml=ucx\n",
    "export OMPI_MCA_btl=self,vader,tcp,smcuda #openib\n",
    "export UCX_RNDV_SCHEME=get_zcopy\n",
    "export UCX_RNDV_THRESH=0\n",
    "export UCX_MAX_RNDV_RAILS=1\n",
    "export UCX_MEMTYPE_CACHE=n\n",
    "```\n",
    "\n",
    "Add the following to qcmd to try it out.\n",
    "\n",
    "`-v CUDA_LAUNCH_BLOCKING=0,\"UCX_TLS='rc,sm,cuda_copy,cuda_ipc'\",OMPI_MCA_pml=ucx, \"OMPI_MCA_btl='self,vader,tcp,smcuda'\",UCX_RNDV_SCHEME=get_zcopy,UCX_RNDV_THRESH=0, UCX_MAX_RNDV_RAILS=1,UCX_MEMTYPE_CACHE=n` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77599428-7fa8-4386-b607-f8fc17f421e3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Resources\n",
    "* MPI\n",
    "    * Cornell's Virtual Workshop [5-part MPI Series](https://cvw.cac.cornell.edu/topics#MPI)\n",
    "    * NCSA's and UIUC's [Introduction to MPI](https://www.hpc-training.org/xsede/moodle/enrol/index.php?id=34) on the [hpc-training.org](https://www.hpc-training.org/xsede/moodle/) HPC-Moodle platform\n",
    "    * XSEDE's [HPC Workshop: MPI](https://www.psc.edu/resources/training/xsede-hpc-workshop-may-2022-mpi/) May 2022 offering\n",
    "* Multi-GPU\n",
    "    * Condensed NVIDIA documentation on [NCCL Fortran API](https://docs.nvidia.com/hpc-sdk/compilers/fortran-cuda-interfaces/index.html#cfnccl-runtime)\n",
    "    * Full NVIDIA [NCCL Documentation](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/index.html)\n",
    "    * [Julich Multi-GPU Tutorial](https://github.com/FZJ-JSC/tutorial-multi-gpu) material as provided at SC21 and ISC22\n",
    "    * Cineca's [OpenACC Tutorial](https://github.com/romerojosh/cineca-openacc-tutorial), Conjugate Gradient solver using CUDA Aware MPI and NCCL\n",
    "    * Jiri Kraus' [Multi-GPU Programming Model](https://github.com/NVIDIA/multi-gpu-programming-models) examples using a Jacobi Solver, from OpenMP to MPI and NCCL/NVSHMEM, with different implementations such as overlapping communication\n",
    "    * Repository of NVIDIA [NCCL testing codes](https://github.com/NVIDIA/nccl-tests)\n",
    "* Other\n",
    "    * [Interoperability examples](https://github.com/OpenACC/openacc-interoperability-examples) with OpenACC\n",
    "    * System [verification of CUDA Aware MPI](https://github.com/muriloboratto/cuda-aware-mpi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
